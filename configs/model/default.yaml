defaults:
  - /model/components/sequence_models@models_config.sequence_model_xz: default
  - /model/components/symbolic_bottlenecks@models_config.symbolic_autoencoder_wrapper_xzx: default
  - /model/components/sequence_models@models_config.sequence_model_zx: default
  - /model/components/symbolic_bottlenecks@models_config.symbolic_autoencoder_wrapper_zxz: default
  - /model/components/discretizers@models_config.discretizer_x: default
  - /model/components/discretizers@models_config.discretizer_z: default

_target_: src.models.sigmae_lit_module_base.SigmaeLitModuleBase

model_params:
  max_x_length: ???
  max_z_length: ???
  d_model: ???
  discretize_x: 1.0
  discretize_z: 1.0
  compile: false # compile model for faster training with pytorch 2.0

optimizer:
  _target_: torch.optim.AdamW
  # _partial_: true # it will return a partial function
  lr: 0.005
  weight_decay: 0.001

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  # _partial_: true # it will return a partial function
  mode: min
  factor: 0.95
  patience: 5
  threshold: 0.1
  threshold_mode: "rel" # rel or abs
  cooldown: 2
  min_lr: 1e-7
  eps: 1e-8
  verbose: True

  scheduler_config:
    interval: "epoch" # The unit of the scheduler's step size. 'step' or 'epoch
    frequency: 1 # corresponds to updating the learning rate after every `frequency` epoch/step
    monitor: ${model.model_params.monitor}
    strict: False
