

_target_: symbolic_bottleneck.auto_reg_wrapper.ImageInputAutoRegWrapper

model:
  _target_: transformers.VisionEncoderDecoderModel.from_encoder_decoder_configs
  encoder_config:
    _target_: transformers.ViTConfig
    attention_probs_dropout_prob: 0.0
    encoder_stride: 16
    hidden_act: "gelu"
    hidden_dropout_prob: 0.0
    hidden_size: ${model.model_params.d_model}
    image_size: ??? #TODO: fill in
    initializer_range: 0.02
    intermediate_size: ${mult_int:${model.model_params.d_model},4} # In vit-gpt2 it's 4 times the hidden size so I defaulted to this
    layer_norm_eps: 1e-12
    model_type: "vit"
    num_attention_heads: 8
    num_channels: 3
    num_hidden_layers: 8
    patch_size: 16
    qkv_bias: true

  decoder_config:
    _target_: transformers.GPT2Config
    activation_function": "gelu_new"
    add_cross_attention": true
    architectures": ["GPT2LMHeadModel"]
    attn_pdrop: 0.1
    embd_pdrop: 0.1
    initializer_range: 0.02
    is_decoder: true
    layer_norm_epsilon: 1e-05
    model_type: "gpt2"
    n_ctx: ${add_int:${model.model_params.max_x_length},${model.model_params.max_z_length}}
    n_embd: ${model.model_params.d_model}
    n_head: 8
    n_inner: null
    n_layer: 8
    n_positions: ${add_int:${model.model_params.max_x_length},${model.model_params.max_z_length}}
    reorder_and_upcast_attn: false
    resid_pdrop: 0.1
    scale_attn_by_inverse_layer_idx: false
    scale_attn_weights: true
    summary_activation: null
    summary_first_dropout: 0.1
    summary_proj_to_labels: true
    summary_type: "cls_index"
    summary_use_proj: true
    vocab_size: ??? # 23 ?

processor:
  _target_: 'transformers.ViTImageProcessor'
  pretrained_model_name_or_path: ???

model_unwrapper: 
  _target_: symbolic_bottleneck.modules.model_unwrapper.transformer_enc_dec_unwrapper.EncoderDecoderUnwrapper

config:
  output_prepending_ids: null
  max_lengths:
    input: 1 # is  this right ?
    output: 45