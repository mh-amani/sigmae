defaults:
  - default
  
model:
  _target_: 'transformers.BartForConditionalGeneration'
  config: 
    _target_: 'transformers.BartConfig'
    d_model: ${model.model_params.d_model}
    encoder_layers: 8
    decoder_layers: 8
    vocab_size: 23
    max_position_embeddings: ${add_int:${model.model_params.max_x_length},${model.model_params.max_z_length}}
    encoder_attention_heads: 4
    decoder_attention_heads: 4
    encoder_ffn_dim: 1024
    decoder_ffn_dim: 1024

# tokenizer:
#   _target_: 'transformers.BartTokenizer.from_pretrained'
#   pretrained_model_name_or_path: "facebook/bart-large"

tokenizer:
  _target_: 'tokenizers.Tokenizer.from_file'
  pretrained_model_name_or_path: ???

model_unwrapper: 
  _target_: symbolic_bottleneck.modules.model_unwrapper.transformer_enc_dec_unwrapper.EncoderDecoderUnwrapper

config:
  output_prepending_ids: null