# @package _global_

defaults:
  - example
  - override /data: text
  - override /data/dataset: text/scan_dataset
  - override /model: bart_bart_times_two
  - override /model/components/discretizers@model.models_config.discretizer_x: gumbelDB
  - override /model/components/discretizers@model.models_config.discretizer_z: gumbelDB

trainer:
  min_epochs: 50
  max_epochs: 100

callbacks:
  supervision_scheduler:
    scheduler_xz: 
      hp_init: 0.0
      hp_end: 0.0
    scheduler_z:
      hp_init: 1.0
      hp_end: 1.0
  model_checkpoint:
    monitor: ${model.model_params.monitor}
  early_stopping:
    monitor: ${model.model_params.monitor}

data:
  supervision_ratio: [0, 1.00]
  batch_size: 210

model:
  optimizer:
    lr: 0.0001
    
  model_params:
    # to be manually set
    x_vocab_size: 17
    z_vocab_size: 10
    max_x_length: 12
    max_z_length: 60
    d_model: 256
    monitor: 'val/zxz/loss'
  
  models_config:
    sequence_model_xz:
      config:
        max_lengths:
          input: ${model.model_params.max_x_length}
          output: ${model.model_params.max_z_length}
      model:
        config:
          vocab_size: ${add_int:${model.model_params.x_vocab_size}, ${model.model_params.z_vocab_size}}
          # max_position_embeddings: ${input_max_position_embeddings}
      tokenizer:
        _target_: transformers.AutoTokenizer.from_pretrained
        pretrained_model_name_or_path: "./data/tokenizers/scan/commands"
        add_special_tokens: true
    
    sequence_model_zx:
      config:
        max_lengths:
          input: ${model.model_params.max_z_length}
          output: ${model.model_params.max_x_length}
      model:
        config:
          vocab_size: ${add_int:${model.model_params.z_vocab_size}, ${model.model_params.x_vocab_size}}
          # max_position_embeddings: ${output_max_position_embeddings}
      tokenizer:
        _target_: transformers.AutoTokenizer.from_pretrained
        pretrained_model_name_or_path: "./data/tokenizers/scan/actions"
        add_special_tokens: true

  scheduler:
    scheduler_config:
      monitor: ${model.model_params.monitor}