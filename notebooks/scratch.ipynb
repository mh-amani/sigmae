{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram: ('a', 'a'), IDF: 0.0\n",
      "Bi-gram: ('a', 'b'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('b', 'd'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('d', 'cd'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('cd', 'c'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('d', 'b'), IDF: 0.6931471805599453\n",
      "Bi-gram: ('b', 'c'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('c', 'a'), IDF: 0.6931471805599453\n",
      "Bi-gram: ('a', 'c'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('c', 'dd'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('dd', 'b'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('b', 'a'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('cc', 'c'), IDF: 1.3862943611198906\n",
      "Bi-gram: ('a', 'd'), IDF: 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "corpus = {\n",
    "    'Doc0': \"a a b d cd c\",\n",
    "    'Doc1': \"d b c a a c\",\n",
    "    'Doc2': \"c dd b a a a\",\n",
    "    'Doc3': \"cc c a a d b\"\n",
    "}\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokenized_docs = {doc: doc_text.split() for doc, doc_text in corpus.items()}\n",
    "\n",
    "# Step 2: Bi-gram Tokenization\n",
    "bi_grams = defaultdict(int)\n",
    "for doc, tokens in tokenized_docs.items():\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bi_gram = (tokens[i], tokens[i + 1])\n",
    "        bi_grams[bi_gram] += 1\n",
    "\n",
    "# Step 3: Compute Document Frequency (DF) for each token\n",
    "document_frequency = defaultdict(int)\n",
    "for bi_gram in bi_grams:\n",
    "    for tokens in tokenized_docs.values():\n",
    "        if bi_gram in zip(tokens, tokens[1:]):\n",
    "            document_frequency[bi_gram] += 1\n",
    "\n",
    "# Step 4: Compute Inverse Document Frequency (IDF) for each token\n",
    "total_docs = len(tokenized_docs)\n",
    "idf = {bi_gram: math.log(total_docs / (document_frequency[bi_gram]))\n",
    "       for bi_gram in bi_grams}\n",
    "\n",
    "# Print Bi-grams and their IDF\n",
    "for bi_gram, idf_value in idf.items():\n",
    "    print(f\"Bi-gram: {bi_gram}, IDF: {idf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency[('a', 'a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9932517730102834"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from vector_quantize_pytorch import VectorQuantize\n",
    "\n",
    "# vq = VectorQuantize(\n",
    "#         dim = self.dictionary_dim,\n",
    "#         codebook_size = self.vocab_size,     # codebook size\n",
    "#         decay = ,             # the exponential moving average decay, lower means the dictionary will change faster\n",
    "#         commitment_weight = kwargs['commitment_weight'],   # the weight on the commitment loss\n",
    "#         use_cosine_sim = True,               # use cosine similarity instead of L2 distance\n",
    "#     )\n",
    "\n",
    "\n",
    "# print(torch.round(vq.codebook, decimals=3))\n",
    "# x = torch.randn(1, 1024, 256)\n",
    "# quantized, indices, commit_loss = vq(x) # (1, 1024, 256), (1, 1024), (1)\n",
    "# print(torch.round(vq.codebook, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.tensor([-0.0627,  0.1373,  0.0616, -1.7994,  0.8853, \n",
    "                  -0.0656,  1.0034,  0.6974,  -0.2919, -0.0456], requires_grad=True)\n",
    "u = torch.argmax(t)\n",
    "u.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of x:\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "Gradients of codebook:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from vector_quantize_pytorch import VectorQuantize\n",
    "from entmax import sparsemax\n",
    "\n",
    "class VQVAEDiscreteLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()      \n",
    "        \n",
    "        self.dictionary = nn.Embedding(17, 20)\n",
    "        self.dist_ord = 2\n",
    "        self.hard = False\n",
    "        self.kernel = nn.Softmax(dim=-1)\n",
    "\n",
    "    def discretize(self, x) -> dict:\n",
    "        probs = self.kernel( - self.codebook_distances(x) / 0.0001)\n",
    "        indices = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        if self.hard:\n",
    "            # Apply STE for hard quantization\n",
    "            quantized = self.dictionary(indices)\n",
    "            quantized = quantized + x - (x).detach()\n",
    "        else:\n",
    "            quantized = torch.matmul(probs, self.dictionary.weight)\n",
    "\n",
    "        return indices, probs, quantized\n",
    "\n",
    "    def codebook_distances(self, x):\n",
    "        x_expanded = x.unsqueeze(2)  # Shape: (batch, length, 1, dim)\n",
    "        dictionary_expanded = self.dictionary.weight.unsqueeze(0).unsqueeze(1)  # Shape: (batch, 1, vocab, dim)\n",
    "        # Compute the squared differences\n",
    "        dist = torch.linalg.vector_norm(x_expanded - dictionary_expanded, ord=self.dist_ord, dim=-1)\n",
    "        return dist\n",
    "    \n",
    "x = torch.randn(1, 10, 20, requires_grad=True)\n",
    "vq = VQVAEDiscreteLayer()\n",
    "indices, probs, quantized = vq.discretize(x)\n",
    "\n",
    "loss = quantized[0][0][0]\n",
    "\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "print(\"Gradients of x:\")\n",
    "print(x.grad)\n",
    "\n",
    "print(\"Gradients of codebook:\")\n",
    "print(vq.dictionary.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 16,  5, 10, 12, 14, 11, 16,  3,  9]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 20])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
