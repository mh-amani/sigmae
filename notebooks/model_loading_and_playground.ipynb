{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/amani/miniconda3/envs/sigmae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from  notebook_utils import run_inference, send_batch_to_device\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### analysis of the model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image):\n",
    "    image = image.cpu().detach().numpy()\n",
    "    image_max = np.max(image)\n",
    "    image_min = np.min(image)\n",
    "    image = (image - image_min) / (image_max - image_min)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(image.transpose(1, 2, 0))\n",
    "    plt.imshow(image)   \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /mnt/dlabscratch1/amani/sigmae/notebooks\n",
      "Instantiating data module <src.data.datamodule.image_lightning_datamodule.ImagePLDataModule>\n",
      "Instantiating model <src.models.sigmae_lit_module_im_to_text.SigmaeLitModuleImageToText>\n"
     ]
    }
   ],
   "source": [
    "path = \"/dlabscratch1/amani/sigmae/logs/train/runs/2024-09-13_20-51-11/\"\n",
    "\n",
    "batch_size = 32\n",
    "model_path = path + \"checkpoints/last.ckpt\"\n",
    "configs_path = path + \".hydra/\"\n",
    "config_name = \"config.yaml\"\n",
    "with hydra.initialize_config_dir(config_dir=configs_path, version_base=\"1.2\"):\n",
    "    config = hydra.compose(config_name=config_name, \n",
    "                           overrides=[f\"data.batch_size={batch_size}\", \n",
    "                                    f\"ckpt_path={model_path}\"\n",
    "                                    ])\n",
    "\n",
    "model, datamodule = run_inference(config)\n",
    "datamodule.processor_z = model.processor_z\n",
    "datamodule.processor_x = None # for image text\n",
    "datamodule.setup('test', processor_z=model.processor_z)\n",
    "\n",
    "test_datamodule = datamodule.test_dataloader()\n",
    "batch = next(iter(test_datamodule))\n",
    "batch = send_batch_to_device(batch, model.device)\n",
    "# output = model.forward(batch['x'], batch['z'], batch['data_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# teacherforced_outputs\n",
    "# noise = torch.randn_like(batch['z'])\n",
    "output_teacher_forced = model.forward(batch['x'], batch['z'], batch ['data_type'])\n",
    "image_patches_teacher_forced = output_teacher_forced[0]['zxz']['id_z']\n",
    "\n",
    "image_teacher_forced = model.fold(image_patches_teacher_forced[:,:-1,...].permute (0, 2, 1)) \n",
    "show_image(image_teacher_forced[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# autoreg_outputs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# output_autoreg = model.symbolic_autoencoder_wrapper_zxz(x_embeds_enc=, teacher_force_z=False)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m output_teacher_forced \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m image_patches_autoreg \u001b[38;5;241m=\u001b[39m output_autoreg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_z\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m image_autoreg \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfold(image_patches_autoreg\u001b[38;5;241m.\u001b[39mpermute (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \n",
      "File \u001b[0;32m/mnt/dlabscratch1/amani/sigmae/src/models/sigmae_lit_module_im_to_text.py:59\u001b[0m, in \u001b[0;36mSigmaeLitModuleImageToText.forward\u001b[0;34m(self, x, z, data_type, stage)\u001b[0m\n\u001b[1;32m     57\u001b[0m z_patches_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer_z\u001b[38;5;241m.\u001b[39mdecoder_embedding(z_patches)\n\u001b[1;32m     58\u001b[0m vit_processed_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor_z(z, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 59\u001b[0m zxz_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_autoencoder_wrapper_zxz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_embeds_enc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit_processed_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mz_embeds_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_patches_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mz_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_force_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzxz\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m zxz_outputs\n\u001b[1;32m     62\u001b[0m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzxz\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m zxz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_z\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/sigmae/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/dlabscratch1/amani/sigmae/symbolic_bottleneck/symbolic_autoencoder_wrapper.py:45\u001b[0m, in \u001b[0;36mSymbolicAutoEncoderWrapper.forward\u001b[0;34m(self, x_ids, x_attention_mask, x_embeds_enc, y_prepending_ids, y_prepending_embeds_enc, y_prepending_embeds_dec, z_ids, z_attention_mask, z_embeds_dec, teacher_force_z, max_y_length, max_z_length)\u001b[0m\n\u001b[1;32m     41\u001b[0m y_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_xy_outputs_to_y_inputs(xy_outputs)\n\u001b[1;32m     42\u001b[0m y_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m y_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     43\u001b[0m     (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m y_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# I don't even know what I'm doing here\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m yz_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_y_to_z\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeds_enc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvector_encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_attention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_embeds_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_embeds_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_force_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_force_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_z_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m quantization_loss \u001b[38;5;241m=\u001b[39m xy_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantization_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m yz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantization_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_y\u001b[39m\u001b[38;5;124m'\u001b[39m: xy_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_z\u001b[39m\u001b[38;5;124m'\u001b[39m: yz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_y\u001b[39m\u001b[38;5;124m'\u001b[39m: xy_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_z\u001b[39m\u001b[38;5;124m'\u001b[39m: yz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit_y\u001b[39m\u001b[38;5;124m'\u001b[39m: xy_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit_z\u001b[39m\u001b[38;5;124m'\u001b[39m: yz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m: yz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_decoder\u001b[39m\u001b[38;5;124m'\u001b[39m: yz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_decoder\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: xy_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: yz_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantization_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: quantization_loss,}\n",
      "File \u001b[0;32m~/miniconda3/envs/sigmae/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/dlabscratch1/amani/sigmae/symbolic_bottleneck/auto_reg_wrapper/abstract_auto_reg_wrapper.py:148\u001b[0m, in \u001b[0;36mAbstractAutoRegWrapper.forward\u001b[0;34m(self, teacher_force_output, max_output_length, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m teacher_force_output:\n\u001b[1;32m    143\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_args_for_fn(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential_forward,\n\u001b[1;32m    145\u001b[0m         exclude\u001b[38;5;241m=\u001b[39m [],\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs_for_forward\n\u001b[1;32m    147\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequential_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_args_for_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteacher_forced_model_forward, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs_for_forward)\n",
      "File \u001b[0;32m/mnt/dlabscratch1/amani/sigmae/symbolic_bottleneck/auto_reg_wrapper/abstract_auto_reg_wrapper.py:279\u001b[0m, in \u001b[0;36mAbstractAutoRegWrapper.sequential_forward\u001b[0;34m(self, input_embeds, max_output_length, output_embeds_enc, output_embeds_dec, input_attention_mask, output_attention_mask)\u001b[0m\n\u001b[1;32m    274\u001b[0m     seq_forward_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_one_step_seq_forward(current_output, step ,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mseq_forward_params)\n\u001b[1;32m    276\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 279\u001b[0m return_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_seq_forward_output_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprend_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mseq_forward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_arguments\n",
      "File \u001b[0;32m/mnt/dlabscratch1/amani/sigmae/symbolic_bottleneck/auto_reg_wrapper/out_cont_auto_reg_wrapper.py:240\u001b[0m, in \u001b[0;36mOutContAutoRegWrapper.return_seq_forward_output_dict\u001b[0;34m(self, step, preprend_length, ids, output_embeds_encs, output_embeds_decs, quantization_loss, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m ids \u001b[38;5;241m=\u001b[39m ids[:, :step]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# cut the tensors to the actual length, remove 0s and 1s.\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m output_embeds_encs \u001b[38;5;241m=\u001b[39m \u001b[43moutput_embeds_encs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    241\u001b[0m output_embeds_decs \u001b[38;5;241m=\u001b[39m output_embeds_decs[:, :step \u001b[38;5;241m+\u001b[39m preprend_length]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: ids,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m: output_embeds_encs,\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_decoder\u001b[39m\u001b[38;5;124m'\u001b[39m: output_embeds_decs,\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantization_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: quantization_loss,\n\u001b[1;32m    248\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# autoreg_outputs\n",
    "\n",
    "# output_autoreg = model.symbolic_autoencoder_wrapper_zxz(x_embeds_enc=, teacher_force_z=False)\n",
    "output_teacher_forced = model.forward(batch['x'], batch['z'], batch ['data_type'], stage='val')\n",
    "image_patches_autoreg = output_autoreg['id_z']\n",
    "image_autoreg = model.fold(image_patches_autoreg.permute (0, 2, 1)) \n",
    "show_image(image_autoreg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9508, 0.9445, 0.9149,  ..., 0.5979, 0.9522, 0.9990],\n",
       "        [0.9682, 0.9511, 0.9717,  ..., 0.8292, 0.8281, 0.8672],\n",
       "        [0.9542, 0.9480, 0.9975,  ..., 1.0013, 1.0097, 0.9951],\n",
       "        ...,\n",
       "        [0.9214, 0.9577, 0.9953,  ..., 0.6729, 0.9145, 1.0043],\n",
       "        [0.9539, 0.9479, 0.9968,  ..., 1.0015, 1.0096, 0.9960],\n",
       "        [0.9544, 0.9478, 0.9967,  ..., 1.0016, 1.0095, 0.9957]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "image_patches_autoreg[:, i, :] - image_patches_teacher_forced[:, i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.6364e-04, -1.0076e-03, -1.3962e-04,  ..., -1.7921e-03,\n",
       "         -1.3089e-03, -1.1953e-03],\n",
       "        [ 5.1957e-03, -8.2200e-03, -3.2266e-03,  ...,  7.7242e-01,\n",
       "          5.8483e-01,  4.2395e-01],\n",
       "        [-8.6363e-04, -1.0076e-03, -1.3965e-04,  ..., -1.7921e-03,\n",
       "         -1.3089e-03, -1.1953e-03],\n",
       "        ...,\n",
       "        [-8.6364e-04, -1.0076e-03, -1.3965e-04,  ..., -1.7921e-03,\n",
       "         -1.3089e-03, -1.1953e-03],\n",
       "        [-8.6365e-04, -1.0076e-03, -1.3962e-04,  ..., -1.7921e-03,\n",
       "         -1.3089e-03, -1.1953e-03],\n",
       "        [-8.6365e-04, -1.0076e-03, -1.3962e-04,  ..., -1.7921e-03,\n",
       "         -1.3089e-03, -1.1953e-03]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches_teacher_forced[:, 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1636, 0.7619, 0.9364,  ..., 0.9752, 1.0048, 0.9996],\n",
       "        [0.1636, 0.7619, 0.9364,  ..., 0.9752, 1.0048, 0.9996],\n",
       "        [0.1636, 0.7619, 0.9364,  ..., 0.9752, 1.0048, 0.9996],\n",
       "        ...,\n",
       "        [0.1636, 0.7619, 0.9364,  ..., 0.9752, 1.0048, 0.9996],\n",
       "        [0.1636, 0.7619, 0.9364,  ..., 0.9752, 1.0048, 0.9996],\n",
       "        [0.1636, 0.7619, 0.9364,  ..., 0.9752, 1.0048, 0.9996]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches_autoreg[:, 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.1801e-04, -9.3409e-04,  5.9456e-06,  ..., -9.6493e-04,\n",
       "          -1.0692e-04, -1.1823e-03],\n",
       "         [-1.4181e-03, -5.3994e-04, -5.9152e-04,  ..., -6.0984e-04,\n",
       "          -5.1760e-04, -9.3542e-04],\n",
       "         [-1.2919e-03, -1.4565e-03, -2.9131e-04,  ..., -1.3669e-03,\n",
       "          -2.5395e-04, -1.2880e-03],\n",
       "         ...,\n",
       "         [-1.5198e-03, -6.0817e-04, -4.9685e-04,  ..., -6.4965e-04,\n",
       "          -4.4664e-04, -7.3398e-04],\n",
       "         [-1.2684e-03, -1.4026e-03, -3.1656e-04,  ..., -1.3567e-03,\n",
       "          -2.7505e-04, -1.0814e-03],\n",
       "         [-1.1158e-03, -1.8900e-03, -5.7086e-04,  ..., -1.9360e-03,\n",
       "          -6.0920e-04, -1.6013e-03]],\n",
       "\n",
       "        [[-7.1799e-04, -9.3410e-04,  5.9605e-06,  ..., -1.0765e-03,\n",
       "          -1.0438e-04, -1.0948e-03],\n",
       "         [-1.4181e-03, -5.3994e-04, -5.9151e-04,  ..., -3.3652e-04,\n",
       "          -5.0855e-04, -8.3730e-04],\n",
       "         [-1.2919e-03, -1.4565e-03, -2.9130e-04,  ..., -1.4874e-03,\n",
       "          -6.9041e-05, -1.0843e-03],\n",
       "         ...,\n",
       "         [-1.6353e-03, -6.7314e-04, -5.8514e-04,  ..., -6.7268e-04,\n",
       "          -4.0154e-04, -1.0061e-03],\n",
       "         [-1.3293e-03, -1.4698e-03, -3.0833e-04,  ..., -1.4160e-03,\n",
       "          -3.3971e-04, -1.1625e-03],\n",
       "         [-9.7759e-04, -1.7131e-03, -7.0783e-04,  ..., -1.6602e-03,\n",
       "          -7.0482e-04, -1.7361e-03]],\n",
       "\n",
       "        [[-7.1801e-04, -9.3410e-04,  5.9307e-06,  ..., -9.6493e-04,\n",
       "          -1.0692e-04, -1.1823e-03],\n",
       "         [-1.4181e-03, -5.3995e-04, -5.9152e-04,  ..., -6.0981e-04,\n",
       "          -5.1758e-04, -9.3541e-04],\n",
       "         [-1.2919e-03, -1.4565e-03, -2.9130e-04,  ..., -1.3669e-03,\n",
       "          -2.5396e-04, -1.2880e-03],\n",
       "         ...,\n",
       "         [-1.4422e-03, -5.9756e-04, -4.9829e-04,  ..., -6.0743e-04,\n",
       "          -4.4434e-04, -9.2747e-04],\n",
       "         [-1.2668e-03, -1.3454e-03, -2.9752e-04,  ..., -1.3058e-03,\n",
       "          -2.9996e-04, -1.2922e-03],\n",
       "         [-1.0636e-03, -1.7841e-03, -5.5149e-04,  ..., -1.7558e-03,\n",
       "          -5.0986e-04, -1.7562e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.1799e-04, -9.3409e-04,  5.9605e-06,  ..., -9.6492e-04,\n",
       "          -1.0692e-04, -1.1823e-03],\n",
       "         [-1.4181e-03, -5.3994e-04, -5.9150e-04,  ..., -6.0981e-04,\n",
       "          -5.1759e-04, -9.3541e-04],\n",
       "         [-1.2919e-03, -1.4565e-03, -2.9131e-04,  ..., -1.3669e-03,\n",
       "          -2.5396e-04, -1.2880e-03],\n",
       "         ...,\n",
       "         [-1.4951e-03, -5.9873e-04, -5.6691e-04,  ..., -6.0844e-04,\n",
       "          -5.0235e-04, -9.6338e-04],\n",
       "         [-1.3232e-03, -1.3681e-03, -2.8225e-04,  ..., -1.3417e-03,\n",
       "          -2.8246e-04, -1.2047e-03],\n",
       "         [-9.9173e-04, -1.8723e-03, -5.9573e-04,  ..., -1.8461e-03,\n",
       "          -5.7447e-04, -1.6647e-03]],\n",
       "\n",
       "        [[-7.1801e-04, -9.3409e-04,  5.9307e-06,  ..., -9.6492e-04,\n",
       "          -1.0692e-04, -1.1823e-03],\n",
       "         [-1.4181e-03, -5.3995e-04, -5.9151e-04,  ..., -6.0982e-04,\n",
       "          -5.1759e-04, -9.3541e-04],\n",
       "         [-1.2919e-03, -1.4566e-03, -2.9129e-04,  ..., -1.3670e-03,\n",
       "          -2.5397e-04, -1.2880e-03],\n",
       "         ...,\n",
       "         [-1.4998e-03, -5.7048e-04, -5.8065e-04,  ..., -6.1831e-04,\n",
       "          -5.1161e-04, -7.9942e-04],\n",
       "         [-1.2201e-03, -1.3750e-03, -3.4739e-04,  ..., -1.3350e-03,\n",
       "          -3.3411e-04, -1.2005e-03],\n",
       "         [-1.0581e-03, -1.8094e-03, -5.8337e-04,  ..., -1.7690e-03,\n",
       "          -5.6443e-04, -1.6878e-03]],\n",
       "\n",
       "        [[-7.1802e-04, -9.3408e-04,  5.9605e-06,  ..., -9.6493e-04,\n",
       "          -1.0692e-04, -1.1823e-03],\n",
       "         [-1.4181e-03, -5.3994e-04, -5.9150e-04,  ..., -6.0982e-04,\n",
       "          -5.1758e-04, -9.3542e-04],\n",
       "         [-1.2919e-03, -1.4565e-03, -2.9130e-04,  ..., -1.3669e-03,\n",
       "          -2.5396e-04, -1.2880e-03],\n",
       "         ...,\n",
       "         [-1.4891e-03, -6.1861e-04, -5.0298e-04,  ..., -6.6150e-04,\n",
       "          -4.5334e-04, -9.7548e-04],\n",
       "         [-1.3169e-03, -1.3824e-03, -2.7934e-04,  ..., -1.3378e-03,\n",
       "          -3.1247e-04, -1.2646e-03],\n",
       "         [-9.8075e-04, -1.8619e-03, -5.7063e-04,  ..., -1.8205e-03,\n",
       "          -5.4274e-04, -1.6796e-03]]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_teacher_forced[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0112,  0.0343,  0.0105,  ..., -0.0282, -0.0285, -0.0084],\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.1376, -0.0494, -0.2219,  ...,  0.1565, -0.0369, -0.0224],\n",
       "         ...,\n",
       "         [-0.0080, -0.0239, -0.0461,  ...,  0.2163, -0.0770,  0.0238],\n",
       "         [-0.0096, -0.0118, -0.0663,  ...,  0.2487, -0.0482,  0.0314],\n",
       "         [-0.0173, -0.0283, -0.0710,  ...,  0.2300, -0.0687,  0.0221]],\n",
       "\n",
       "        [[ 0.0112,  0.0343,  0.0105,  ..., -0.0282, -0.0285, -0.0084],\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.1376, -0.0494, -0.2219,  ...,  0.1565, -0.0369, -0.0224],\n",
       "         ...,\n",
       "         [-0.0080, -0.0239, -0.0461,  ...,  0.2163, -0.0770,  0.0238],\n",
       "         [-0.0096, -0.0118, -0.0663,  ...,  0.2487, -0.0482,  0.0314],\n",
       "         [-0.0173, -0.0283, -0.0710,  ...,  0.2300, -0.0687,  0.0221]],\n",
       "\n",
       "        [[ 0.0112,  0.0343,  0.0105,  ..., -0.0282, -0.0285, -0.0084],\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.1376, -0.0494, -0.2219,  ...,  0.1565, -0.0369, -0.0224],\n",
       "         ...,\n",
       "         [-0.0080, -0.0239, -0.0461,  ...,  0.2163, -0.0770,  0.0238],\n",
       "         [-0.0096, -0.0118, -0.0663,  ...,  0.2487, -0.0482,  0.0314],\n",
       "         [-0.0173, -0.0283, -0.0710,  ...,  0.2300, -0.0687,  0.0221]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0112,  0.0343,  0.0105,  ..., -0.0282, -0.0285, -0.0084],\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.1376, -0.0494, -0.2219,  ...,  0.1565, -0.0369, -0.0224],\n",
       "         ...,\n",
       "         [-0.0080, -0.0239, -0.0461,  ...,  0.2163, -0.0770,  0.0238],\n",
       "         [-0.0096, -0.0118, -0.0663,  ...,  0.2487, -0.0482,  0.0314],\n",
       "         [-0.0173, -0.0283, -0.0710,  ...,  0.2300, -0.0687,  0.0221]],\n",
       "\n",
       "        [[ 0.0112,  0.0343,  0.0105,  ..., -0.0282, -0.0285, -0.0084],\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.1376, -0.0494, -0.2219,  ...,  0.1565, -0.0369, -0.0224],\n",
       "         ...,\n",
       "         [-0.0080, -0.0239, -0.0461,  ...,  0.2163, -0.0770,  0.0238],\n",
       "         [-0.0096, -0.0118, -0.0663,  ...,  0.2487, -0.0482,  0.0314],\n",
       "         [-0.0173, -0.0283, -0.0710,  ...,  0.2300, -0.0687,  0.0221]],\n",
       "\n",
       "        [[ 0.0112,  0.0343,  0.0105,  ..., -0.0282, -0.0285, -0.0084],\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.1376, -0.0494, -0.2219,  ...,  0.1565, -0.0369, -0.0224],\n",
       "         ...,\n",
       "         [-0.0080, -0.0239, -0.0461,  ...,  0.2163, -0.0770,  0.0238],\n",
       "         [-0.0096, -0.0118, -0.0663,  ...,  0.2487, -0.0482,  0.0314],\n",
       "         [-0.0173, -0.0283, -0.0710,  ...,  0.2300, -0.0687,  0.0221]]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_autoreg['vector_decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.1464, -0.1128, -0.0843,  ...,  0.0240, -0.1442,  0.0131],\n",
       "         [-0.0494, -0.0887, -0.0656,  ...,  0.0033, -0.1296, -0.0229],\n",
       "         ...,\n",
       "         [-0.0494, -0.0888, -0.0660,  ...,  0.0035, -0.1293, -0.0232],\n",
       "         [-0.0493, -0.0888, -0.0657,  ...,  0.0033, -0.1295, -0.0229],\n",
       "         [-0.1815, -0.0706, -0.1354,  ...,  0.0694, -0.1277,  0.0426]],\n",
       "\n",
       "        [[-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.0760, -0.1037, -0.0664,  ..., -0.0266, -0.1419, -0.0392],\n",
       "         [-0.0882, -0.0762, -0.0999,  ...,  0.0243, -0.1264,  0.0083],\n",
       "         ...,\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0034, -0.1296, -0.0228],\n",
       "         [-0.0492, -0.0888, -0.0658,  ...,  0.0034, -0.1295, -0.0228],\n",
       "         [-0.0470, -0.0870, -0.0684,  ...,  0.0274, -0.1314, -0.0276]],\n",
       "\n",
       "        [[-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.0493, -0.0887, -0.0656,  ...,  0.0033, -0.1294, -0.0227],\n",
       "         [-0.1467, -0.0419, -0.1303,  ...,  0.0816, -0.1093, -0.0091],\n",
       "         ...,\n",
       "         [-0.0491, -0.0885, -0.0654,  ...,  0.0034, -0.1295, -0.0228],\n",
       "         [-0.0491, -0.0887, -0.0655,  ...,  0.0034, -0.1295, -0.0228],\n",
       "         [-0.0493, -0.0891, -0.0662,  ...,  0.0029, -0.1294, -0.0227]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.0779, -0.0889, -0.0975,  ...,  0.0171, -0.1333, -0.0098],\n",
       "         [-0.0492, -0.0886, -0.0655,  ...,  0.0033, -0.1297, -0.0227],\n",
       "         ...,\n",
       "         [-0.0459, -0.0852, -0.0683,  ...,  0.0118, -0.1287, -0.0088],\n",
       "         [-0.0493, -0.0888, -0.0656,  ...,  0.0032, -0.1297, -0.0230],\n",
       "         [-0.1880, -0.1239, -0.0700,  ...,  0.0272, -0.1508, -0.0249]],\n",
       "\n",
       "        [[-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.0494, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0229],\n",
       "         [-0.0842, -0.0496, -0.1875,  ..., -0.0357, -0.0724, -0.0457],\n",
       "         ...,\n",
       "         [-0.0472, -0.0869, -0.0663,  ...,  0.0139, -0.1285, -0.0236],\n",
       "         [-0.0492, -0.0887, -0.0658,  ...,  0.0034, -0.1295, -0.0229],\n",
       "         [-0.0526, -0.0986, -0.0781,  ...,  0.0029, -0.1346, -0.0254]],\n",
       "\n",
       "        [[-0.0492, -0.0887, -0.0655,  ...,  0.0032, -0.1295, -0.0227],\n",
       "         [-0.0492, -0.0886, -0.0655,  ...,  0.0032, -0.1294, -0.0228],\n",
       "         [-0.0484, -0.0879, -0.0661,  ...,  0.0041, -0.1299, -0.0215],\n",
       "         ...,\n",
       "         [-0.0492, -0.0888, -0.0656,  ...,  0.0033, -0.1296, -0.0229],\n",
       "         [-0.0492, -0.0887, -0.0655,  ...,  0.0034, -0.1296, -0.0228],\n",
       "         [-0.1029, -0.0934, -0.0746,  ...,  0.0288, -0.1300,  0.0355]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_teacher_forced[0]['zxz']['vector_decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=48, bias=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.discretizer_z.linear_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 42\n",
    "dataset='val'\n",
    "\n",
    "# print('supervised model 0.04:')\n",
    "# forward(model_sup_004, datamodule_sup_004, id, dataset=dataset)\n",
    "# print('curriculum model 0.04:')\n",
    "# forward(model_cur_004, datamodule_cur_004, id, dataset=dataset)\n",
    "\n",
    "################################################ single model ##########################################################\n",
    "print('id:{} dataset:{}'.format(id, dataset))\n",
    "forward(model, datamodule, id, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neat_matrix(model.disc_x.dictionary.weight)\n",
    "# torch.linalg.vector_norm(model.disc_x.dictionary.weight, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.isnan().any())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = next(iter(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 20\n",
    "model = model\n",
    "datamodule = datamodule\n",
    "print(datamodule.data_train[id])\n",
    "pred_from_sample(model, datamodule, 'turn left after walk opposite right', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'walk opposite right', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'turn left', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'walk opposite right after turn left', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'walk opposite right after turn left and walk opposite right after turn left', print_x_or_z='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model\n",
    "z_vocab = ['I_RUN', 'I_JUMP', 'I_WALK', 'I_LOOK', 'I_TURN_LEFT', 'I_TURN_RIGHT', ]\n",
    "x_vocab = ['left', 'thrice', 'after','and', 'right', 'turn', 'run', 'walk', 'look', 'jump', 'opposite', \n",
    "        'around', 'twice']\n",
    "\n",
    "pred_from_sample(model, 'I_JUMP', print_x_or_z='x')\n",
    "pred_from_sample(model, 'I_JUMP I_JUMP', print_x_or_z='x')\n",
    "pred_from_sample(model, 'I_JUMP I_JUMP I_JUMP', print_x_or_z='x')\n",
    "pred_from_sample(model, 'I_JUMP I_JUMP I_JUMP I_JUMP', print_x_or_z='x')\n",
    "\n",
    "\n",
    "pred_from_sample(model, 'jump and jump', print_x_or_z='z')\n",
    "pred_from_sample(model, 'jump twice', print_x_or_z='z')\n",
    "pred_from_sample(model, 'jump twice and jump', print_x_or_z='z')\n",
    "pred_from_sample(model, 'jump and jump and jump', print_x_or_z='z')\n",
    "pred_from_sample(model, 'jump and jump and jump and jump', print_x_or_z='z')\n",
    "pred_from_sample(model, 'jump around left', print_x_or_z='z')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_sup_004\n",
    "sentence = 'walk twice and turn left twice'\n",
    "pred_from_sample(model, sentence, print_x_or_z='z')\n",
    "model = model_cur_004\n",
    "pred_from_sample(model, sentence, print_x_or_z='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ameliorated val datapoints from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_z_ids_cur, correct_x_ids_cur = correct_val_predictions(model_cur_004, datamodule_cur_004)\n",
    "correct_z_ids_sup, correct_x_ids_sup = correct_val_predictions(model_sup_004, datamodule_sup_004)\n",
    "intersection = list(set(correct_z_ids_cur).intersection(set(correct_x_ids_cur)))\n",
    "lost_z_ids_by_cur = list(set(correct_z_ids_sup).difference(set(correct_z_ids_cur)))\n",
    "found_z_ids_by_cur = list(set(correct_z_ids_cur).difference(set(correct_z_ids_sup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('curriculum accuracy: ', len(correct_z_ids_cur)/len(datamodule_cur_004.data_val))\n",
    "print('supervised accuracy: ', len(correct_z_ids_sup)/len(datamodule_sup_004.data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of lost z ids by curr: ',len(lost_z_ids_by_cur))\n",
    "print('size of found z ids by curr: ',len(found_z_ids_by_cur))\n",
    "\n",
    "i = 20\n",
    "print('supervised model 0.04:')\n",
    "forward(model_sup_004, datamodule_sup_004, lost_z_ids_by_cur[i])\n",
    "print('curriculum model 0.04:')\n",
    "forward(model_cur_004, datamodule_cur_004, lost_z_ids_by_cur[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 50\n",
    "print('supervised model 0.04:')\n",
    "forward(model_sup_004, datamodule_sup_004, found_z_ids_by_cur[i])\n",
    "print('curriculum model 0.04:')\n",
    "forward(model_cur_004, datamodule_cur_004, found_z_ids_by_cur[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_z_ids_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_z_ids_cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why is the fully supervised model with perfect loss not working well on zxz and xzx?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "path=\"'/dlabdata1/masani/blocks/logs/training/runs/scan/suponly-[0.99, 0.9]-gpt2_gpt2-vqvae/2023-11-15_13-48-10/checkpoints/last.ckpt'\"\n",
    "\n",
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    config_sup = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=scan\", \"datamodule.dataset_parameters.supervision_ratio=[0.99,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\"\n",
    "                                      ])\n",
    "\n",
    "model_sup , datamodule_sup = run_inference(config_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 20\n",
    "model = model_sup\n",
    "datamodule = datamodule_sup\n",
    "print(datamodule_sup.data_train[id])\n",
    "forward(model_sup, datamodule_sup, id, dataset='train')\n",
    "pred_from_sample(model, datamodule, 'turn left after walk opposite right', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'walk opposite right', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'turn left', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'walk opposite right after turn left', print_x_or_z='z')\n",
    "pred_from_sample(model, datamodule, 'walk opposite right after turn left and walk opposite right after turn left', print_x_or_z='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.disc_x.dictionary.weight)\n",
    "dictionary_cosine_sim(model, alphabet='x', bins=100)\n",
    "dictionary_inner_prod_sim(model, alphabet='x', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_z_ids, correct_x_ids = correct_val_predictions(model_sup, datamodule_sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## model weights analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of functions\n",
    "def dictionary_cosine_sim(model, alphabet='x', bins=100):\n",
    "    if alphabet == 'x':\n",
    "        kernel = model.disc_x.state_dict()['dictionary.weight'].cpu().numpy()\n",
    "    elif alphabet == 'z':\n",
    "        kernel = model.disc_z.state_dict()['dictionary.weight'].cpu().numpy()\n",
    "    # cosine similarity\n",
    "    inner_prods = kernel.dot(kernel.T)\n",
    "    lengths = np.linalg.norm(kernel, axis=1)\n",
    "    length_matrix = np.outer(lengths, lengths)\n",
    "    kernel = np.round(inner_prods / length_matrix, decimals=2)\n",
    "    u = plt.hist(kernel.flatten(), bins=bins)\n",
    "    plt.show()\n",
    "    return kernel, u\n",
    "\n",
    "\n",
    "def dictionary_inner_prod_sim(model, alphabet='x', bins=100):\n",
    "    if alphabet == 'x':\n",
    "        kernel = model.disc_x.state_dict()['dictionary.weight'].cpu().numpy()\n",
    "    elif alphabet == 'z':\n",
    "        kernel = model.disc_z.state_dict()['dictionary.weight'].cpu().numpy()\n",
    "    inner_prods = kernel.dot(kernel.T)\n",
    "    kernel = np.round(inner_prods, decimals=2)\n",
    "    u = plt.hist(kernel.flatten(), bins=bins)\n",
    "    plt.show()\n",
    "    return kernel, u\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel, u = dictionary_inner_prod_sim(model, alphabet='z', bins=100)\n",
    "print_neat_matrix(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "path=\"'/dlabdata1/masani/blocks/logs/training/runs/scan/suponly-[0.04, 0.9]-gpt2_gpt2-vqvae/2023-10-15_14-51-47/checkpoints/last.ckpt'\"\n",
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    config_sup_004 = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=scan\", \"datamodule.dataset_parameters.supervision_ratio=[0.04,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\"\n",
    "                                      ])\n",
    "\n",
    "model_sup_004 , datamodule_sup_004 = run_inference(config_sup_004)\n",
    "\n",
    "batch_size = 10\n",
    "path=\"'/dlabdata1/masani/blocks/logs/training/runs/scan/curriculum-[0.04, 0.9]-gpt2_gpt2-vqvae/2023-10-17_14-13-22/checkpoints/last.ckpt'\"\n",
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    config_cur_004 = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=scan\", \"datamodule.dataset_parameters.supervision_ratio=[0.04,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\"\n",
    "                                      ])\n",
    "\n",
    "model_cur_004, datamodule_cur_004 = run_inference(config_cur_004)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annot = False\n",
    "for alph in ['x', 'z']:\n",
    "    if alph == 'x':\n",
    "        bin = 100\n",
    "        tokenizer=model.collator.tokenizer_x\n",
    "    elif alph == 'z':\n",
    "        bin = 20\n",
    "        tokenizer=model.collator.tokenizer_z\n",
    "    print(alph)\n",
    "    for type in ['sup', 'cur']:\n",
    "        print(type)\n",
    "        if type == 'sup':\n",
    "            model = model_sup_004\n",
    "        elif type == 'cur':\n",
    "            model = model_cur_004\n",
    "        cosine_kernel, cosine_u = dictionary_cosine_sim(model, alphabet=alph, bins=bin)\n",
    "\n",
    "        sorted_vocab = sorted(tokenizer.get_vocab().items(), key=lambda x: x[1])\n",
    "        sorted_vocab = [x[0] for x in sorted_vocab]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(alph+' '+type+' cosine similarity')\n",
    "        sns.heatmap(cosine_kernel, cmap=\"coolwarm\", annot=annot, xticklabels=sorted_vocab, yticklabels=sorted_vocab)\n",
    "        plt.show()\n",
    "\n",
    "        inner_prod_kernel, inner_prod_u = dictionary_inner_prod_sim(model, alphabet=alph, bins=bin)\n",
    "        plt.figure()\n",
    "        plt.title(alph+' '+type+' inner product similarity')\n",
    "        sns.heatmap(inner_prod_kernel, cmap=\"coolwarm\", annot=annot, xticklabels=sorted_vocab, yticklabels=sorted_vocab)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# cos_sup_004, u_sup_004 = dictionary_cosine_sim(model_sup_004, bins=bin)\n",
    "# cos_cur_004, u_cur_004 = dictionary_cosine_sim(model_cur_004, bins=bin)\n",
    "# sns.heatmap(cos_sup_004, cmap=\"coolwarm\", annot=True)\n",
    "# plt.show()\n",
    "# sns.heatmap(cos_cur_004, cmap=\"coolwarm\", annot=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading other datamodules and sampling outputs for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "from src import utils\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from src.utils.metrics import pad_label_label\n",
    "# from src.utils import general_helpers\n",
    "from typing import List\n",
    "import seaborn as sns\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from pytorch_lightning.loggers.logger import Logger\n",
    "\n",
    "import src.utils.general as utils\n",
    "# Load config\n",
    "import hydra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log = utils.get_pylogger(__name__)\n",
    "\n",
    "configs_path = \"../configs\"\n",
    "config_name = \"inference_root.yaml\"\n",
    "work_dir='/dlabdata1/masani/blocks/'\n",
    "batch_size = 32\n",
    "path=\"'/dlabdata1/masani/blocks/logs/training/runs/scan/suponly-[0.99, 0.9]-gpt2_gpt2-vqvae/2023-11-17_15-54-41/checkpoints/last.ckpt'\"\n",
    "\n",
    "def load_datamodule(config: DictConfig):\n",
    "    # assert config.output_dir is not None, \"Path to the directory in which the predictions will be written must be given\"\n",
    "    # config.output_dir = general_helpers.get_absolute_path(config.output_dir)\n",
    "    # log.info(f\"Output directory: {config.output_dir}\")\n",
    "\n",
    "    # Set seed for random number generators in PyTorch, Numpy and Python (random)\n",
    "    if config.get(\"seed\"):\n",
    "        pl.seed_everything(config.seed, workers=True)\n",
    "    \n",
    "    # print current working directory\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "    print(f\"Instantiating data module <{config.datamodule._target_}>\")\n",
    "    datamodule: LightningDataModule = hydra.utils.instantiate(config.datamodule, _recursive_=False)\n",
    "\n",
    "    return datamodule\n",
    "\n",
    "def print_sample(datamodule, num_sample):\n",
    "    ids = np.random.randint(0, len(datamodule.data_val), num_sample)\n",
    "    for id in ids:\n",
    "        print('id: ', id)\n",
    "        print('x: ', datamodule.data_val[int(id)]['x'])\n",
    "        print('z: ', datamodule.data_val[int(id)]['z'])\n",
    "        print('_____________________________________________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    cnfig = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=cfq\", \"datamodule.dataset_parameters.supervision_ratio=[0.04,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\",\n",
    "                                      \"work_dir='/dlabdata1/masani/blocks/'\"\n",
    "                                      ])\n",
    "datamodule = load_datamodule(cnfig)\n",
    "print_sample(datamodule, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    cnfig = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=cogs\", \"datamodule.dataset_parameters.supervision_ratio=[0.04,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\",\n",
    "                                      \"work_dir='/dlabdata1/masani/blocks/'\"\n",
    "                                      ])\n",
    "datamodule = load_datamodule(cnfig)\n",
    "print_sample(datamodule, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    cnfig = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=pcfg_set\", \"datamodule.dataset_parameters.supervision_ratio=[0.04,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\",\n",
    "                                      \"work_dir='/dlabdata1/masani/blocks/'\"\n",
    "                                      ])\n",
    "datamodule = load_datamodule(cnfig)\n",
    "print_sample(datamodule, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    cnfig = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=sfst\", \"datamodule.dataset_parameters.supervision_ratio=[0.04,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\"\n",
    "                                      ])\n",
    "datamodule = load_datamodule(cnfig)\n",
    "print_sample(datamodule, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=configs_path, version_base=\"1.2\"):\n",
    "    cnfig = hydra.compose(config_name=config_name, \n",
    "                           overrides=[\"+experiment/inference=inference\", \n",
    "                                      \"datamodule=scan\", \"datamodule.dataset_parameters.supervision_ratio=[0.04,0.9]\",\n",
    "                                      \"trainer.devices=[1]\", \n",
    "                                      \"training_type=suponly\", \n",
    "                                      f\"datamodule.dataset_parameters.batch_size={batch_size}\", \n",
    "                                      \"sequence_to_sequence_model_key=gpt2_gpt2\", \n",
    "                                      \"discretizer_key=vqvae\",\n",
    "                                      f\"model.checkpoint_path={path}\"\n",
    "                                      ])\n",
    "datamodule = load_datamodule(cnfig)\n",
    "print_sample(datamodule, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
